{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gusdn\\anaconda3\\envs\\aienv\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(y.values[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = enc.transform(y.values[:,np.newaxis]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], Y[:60000], Y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, W):\n",
    "    K = np.size(W, 1)\n",
    "    A = np.exp(X @ W)\n",
    "    B = np.diag(1 / (np.reshape(A @ np.ones((K,1)), -1)))\n",
    "    Y = B @ A\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, T, W, lmbda=0.01):\n",
    "    epsilon = 1e-5\n",
    "    N = len(T)\n",
    "    K = np.size(T, 1)\n",
    "    cost = - (1/N) * np.ones((1,N)) @ (np.multiply(np.log(softmax(X, W) + epsilon), T)) @ np.ones((K,1))\n",
    "    regularization = lmbda * np.linalg.norm(W)\n",
    "    return cost + regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W):\n",
    "    return np.argmax((X @ W), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gd(X, T, W, learning_rate, iterations, batch_size):\n",
    "    N = len(T)\n",
    "    cost_history = np.zeros((iterations,1))\n",
    "    shuffled_indices = np.random.permutation(N)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    T_shuffled = T[shuffled_indices]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        j = i % N\n",
    "        X_batch = X_shuffled[j:j+batch_size]\n",
    "        T_batch = T_shuffled[j:j+batch_size]\n",
    "        # batch가 epoch 경계를 넘어가는 경우, 앞 부분으로 채워줌\n",
    "        if X_batch.shape[0] < batch_size:\n",
    "            X_batch = np.vstack((X_batch, X_shuffled[:(batch_size - X_batch.shape[0])]))\n",
    "            T_batch = np.vstack((T_batch, T_shuffled[:(batch_size - T_batch.shape[0])]))\n",
    "        W = W - (learning_rate/batch_size) * (X_batch.T @ (softmax(X_batch, W) - T_batch))\n",
    "        cost_history[i] = compute_cost(X_batch, T_batch, W)\n",
    "        if i % 1000 == 0:\n",
    "            print(cost_history[i][0])\n",
    "\n",
    "    return (cost_history, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost is: 2.302485097993717 \n",
      "\n",
      "2.2719144775656384\n",
      "0.5581339617854693\n",
      "0.36994635795019265\n",
      "0.4378231715749116\n",
      "0.4051113802075499\n",
      "0.3296834200796072\n",
      "0.38825484409740085\n",
      "0.2583812132099521\n",
      "0.3345958471344481\n",
      "0.31185848022694457\n",
      "0.4118494940602113\n",
      "0.38286540452624435\n",
      "0.299782641909314\n",
      "0.4332830147807972\n",
      "0.47185934497704435\n",
      "0.26139482389151425\n",
      "0.46356617355781726\n",
      "0.2665022159124113\n",
      "0.36315558267444137\n",
      "0.42030793022242996\n",
      "0.31663105117213525\n",
      "0.3841232337705601\n",
      "0.2461436780513172\n",
      "0.26898146575920107\n",
      "0.4529290936213825\n",
      "0.3919619237912696\n",
      "0.3121794668165689\n",
      "0.25900451504139377\n",
      "0.42351204606872694\n",
      "0.5813083139018221\n",
      "0.2128940011207524\n",
      "0.25655905784218197\n",
      "0.493615527335473\n",
      "0.5786210490572523\n",
      "0.3734278168530569\n",
      "0.34130509788487245\n",
      "0.2874839383521016\n",
      "0.37461915956504327\n",
      "0.45245585957358614\n",
      "0.25245359532132317\n",
      "0.20719249783379717\n",
      "0.37958554020932267\n",
      "0.29543326505011086\n",
      "0.47833371300321537\n",
      "0.32165249512141714\n",
      "0.4782409881763794\n",
      "0.40182837554678585\n",
      "0.38183041920520444\n",
      "0.38111871731313435\n",
      "0.2749222496853956\n"
     ]
    }
   ],
   "source": [
    "X = np.hstack((np.ones((np.size(X_train, 0),1)),X_train))\n",
    "T = y_train\n",
    "\n",
    "K = np.size(T, 1)\n",
    "M = np.size(X, 1)\n",
    "W = np.zeros((M,K))\n",
    "\n",
    "iterations = 50000\n",
    "learning_rate = 0.01\n",
    "\n",
    "initial_cost = compute_cost(X, T, W)\n",
    "\n",
    "print(\"Initial Cost is: {} \\n\".format(initial_cost[0][0]))\n",
    "\n",
    "(cost_history, W_optimal) = batch_gd(X, T, W, learning_rate, iterations, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9164\n"
     ]
    }
   ],
   "source": [
    "## Accuracy\n",
    "X_ = np.hstack((np.ones((np.size(X_test, 0),1)),X_test))\n",
    "T_ = y_test\n",
    "y_pred = predict(X_, W_optimal)\n",
    "score = float(sum(y_pred == np.argmax(T_, axis=1)))/ float(len(y_test))\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def batch_gd(X, T, W, learning_rate, iterations, batch_size):\n",
    "    N = len(T)\n",
    "    cost_history = np.zeros((iterations,1))\n",
    "    valid_cost_history = np.zeros((iterations,1))\n",
    "    shuffled_indices = np.random.permutation(N)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    T_shuffled = T[shuffled_indices]\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_shuffled,T_shuffled, test_size=0.2, random_state=42)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        j = i % N\n",
    "        X_batch = X_train[j:j+batch_size]\n",
    "        T_batch = y_train[j:j+batch_size]\n",
    "        X_valid_batch = X_valid[j:j+batch_size]\n",
    "        T_valid_batch = y_valid[j:j+batch_size]\n",
    "        # batch가 epoch 경계를 넘어가는 경우, 앞 부분으로 채워줌\n",
    "        if X_batch.shape[0] < batch_size:\n",
    "            X_batch = np.vstack((X_batch, X_train[:(batch_size - X_batch.shape[0])]))\n",
    "            T_batch = np.vstack((T_batch, y_train[:(batch_size - T_batch.shape[0])]))\n",
    "            X_valid_batch = np.vstack((X_valid_batch, X_valid[:(batch_size - X_valid_batch.shape[0])]))\n",
    "            T_valid_batch = np.vstack((T_valid_batch, y_valid[:(batch_size - T_valid_batch.shape[0])]))\n",
    "        W = W - (learning_rate/batch_size) * (X_batch.T @ (softmax(X_batch, W) - T_batch))\n",
    "        cost_history[i] = compute_cost(X_batch, T_batch, W)\n",
    "        valid_cost_history[i] = compute_cost(X_valid_batch, T_valid_batch, W)\n",
    "        if i % 1000 == 0:\n",
    "            print('train:',cost_history[i][0])\n",
    "            print('valid:',valid_cost_history[i][0])\n",
    "\n",
    "    return (cost_history, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost is: 2.302485097993717 \n",
      "\n",
      "train: 2.2777930549384697\n",
      "valid: 2.292026605430034\n",
      "train: 0.6089292390648721\n",
      "valid: 0.7498610341437\n",
      "train: 0.44824597833221513\n",
      "valid: 0.4741166375608433\n",
      "train: 0.48665880843778436\n",
      "valid: 0.6316495378605207\n",
      "train: 0.37404612703207557\n",
      "valid: 0.3878211646949781\n",
      "train: 0.46966442214305426\n",
      "valid: 0.40924401690418455\n",
      "train: 0.4402473956114526\n",
      "valid: 0.5411806114040938\n",
      "train: 0.269679294753382\n",
      "valid: 0.3956813679790134\n",
      "train: 0.25996253879236064\n",
      "valid: 0.5369589723127759\n",
      "train: 0.379854091253051\n",
      "valid: 0.45614242927643606\n",
      "train: 0.5018239164638626\n",
      "valid: 0.5327533185378526\n",
      "train: 0.3402549582786679\n",
      "valid: 0.7386878762037165\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m initial_cost \u001b[39m=\u001b[39m compute_cost(X, T, W)\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInitial Cost is: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(initial_cost[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]))\n\u001b[1;32m---> 15\u001b[0m (cost_history, W_optimal) \u001b[39m=\u001b[39m batch_gd(X, T, W, learning_rate, iterations, \u001b[39m64\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[31], line 26\u001b[0m, in \u001b[0;36mbatch_gd\u001b[1;34m(X, T, W, learning_rate, iterations, batch_size)\u001b[0m\n\u001b[0;32m     24\u001b[0m W \u001b[39m=\u001b[39m W \u001b[39m-\u001b[39m (learning_rate\u001b[39m/\u001b[39mbatch_size) \u001b[39m*\u001b[39m (X_batch\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m (softmax(X_batch, W) \u001b[39m-\u001b[39m T_batch))\n\u001b[0;32m     25\u001b[0m cost_history[i] \u001b[39m=\u001b[39m compute_cost(X_batch, T_batch, W)\n\u001b[1;32m---> 26\u001b[0m valid_cost_history[i] \u001b[39m=\u001b[39m compute_cost(X_valid_batch, T_valid_batch, W)\n\u001b[0;32m     27\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     28\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtrain:\u001b[39m\u001b[39m'\u001b[39m,cost_history[i][\u001b[39m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[25], line 5\u001b[0m, in \u001b[0;36mcompute_cost\u001b[1;34m(X, T, W, lmbda)\u001b[0m\n\u001b[0;32m      3\u001b[0m N \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(T)\n\u001b[0;32m      4\u001b[0m K \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msize(T, \u001b[39m1\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m cost \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m (\u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49mN) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mones((\u001b[39m1\u001b[39m,N)) \u001b[39m@\u001b[39m (np\u001b[39m.\u001b[39mmultiply(np\u001b[39m.\u001b[39mlog(softmax(X, W) \u001b[39m+\u001b[39m epsilon), T)) \u001b[39m@\u001b[39m np\u001b[39m.\u001b[39mones((K,\u001b[39m1\u001b[39m))\n\u001b[0;32m      6\u001b[0m regularization \u001b[39m=\u001b[39m lmbda \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(W)\n\u001b[0;32m      7\u001b[0m \u001b[39mreturn\u001b[39;00m cost \u001b[39m+\u001b[39m regularization\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "X = np.hstack((np.ones((np.size(X_train, 0),1)),X_train))\n",
    "T = y_train\n",
    "\n",
    "K = np.size(T, 1)\n",
    "M = np.size(X, 1)\n",
    "W = np.zeros((M,K))\n",
    "\n",
    "iterations = 50000\n",
    "learning_rate = 0.01\n",
    "\n",
    "initial_cost = compute_cost(X, T, W)\n",
    "\n",
    "print(\"Initial Cost is: {} \\n\".format(initial_cost[0][0]))\n",
    "\n",
    "(cost_history, W_optimal) = batch_gd(X, T, W, learning_rate, iterations, 64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
